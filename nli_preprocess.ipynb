{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "jd9LS-R3IhIS",
        "outputId": "f4231ef1-a80f-4d02-e3f0-2c1d13391b52"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from transformers import DataProcessor, InputExample, InputFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "36qk8EK3IhIX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n",
            "101 102 0 100\n",
            "512\n"
          ]
        }
      ],
      "source": [
        "bert_model_type = 'bert-base-uncased'\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_type)\n",
        "cls_token = tokenizer.cls_token\n",
        "sep_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "print(cls_token, sep_token, pad_token, unk_token)\n",
        "\n",
        "cls_token_idx = tokenizer.cls_token_id\n",
        "sep_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "print(cls_token_idx, sep_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "label_conversion = {'n':0, #neutral\n",
        "'e':1, #entailment\n",
        "'c':2} #contradiction\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes[bert_model_type]\n",
        "print(max_input_length)\n",
        "\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iZUFIqLWIhIY"
      },
      "outputs": [],
      "source": [
        "def read_jsonl(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            lines = f.readlines()\n",
        "            return [json.loads(line) for line in lines]\n",
        "            \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2Nm3GGlNIhIZ"
      },
      "outputs": [],
      "source": [
        "def create_examples(filename):\n",
        "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        data = read_jsonl(filename)\n",
        "        for (i, line) in enumerate(data):\n",
        "            guid = \"%s-%s\" % (\"anli-bert-tf\", i)\n",
        "            premise = line['context'] \n",
        "            hypothesis = line['hypothesis']\n",
        "            label = line['label']\n",
        "            examples.append(InputExample(guid=guid, text_a=premise, text_b=hypothesis, label=label))\n",
        "        return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fIrHhYAmIhIZ"
      },
      "outputs": [],
      "source": [
        "#Get list of 0s \n",
        "def get_sent1_token_type(sent):\n",
        "    try:\n",
        "        return [0]* len(sent)\n",
        "    except:\n",
        "        return []\n",
        "#Get list of 1s\n",
        "def get_sent2_token_type(sent):\n",
        "    try:\n",
        "        return [1]* len(sent)\n",
        "    except:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fejYDzBSIhIa"
      },
      "outputs": [],
      "source": [
        "def pad_sequence(sequence, max_seq_length=max_input_length, pad_token=pad_token):\n",
        "    ''' \n",
        "    Pads the sequence to the max_seq_length.\n",
        "    '''\n",
        "    #sequence = sequence.split(\" \")\n",
        "    sequence = sequence[:max_seq_length]\n",
        "    sequence = sequence + [pad_token]*(max_seq_length - len(sequence))\n",
        "    return sequence\n",
        "\n",
        "def pad_attention_mask(attention_mask, max_seq_length=max_input_length):\n",
        "    ''' \n",
        "    Pads the attention mask to the max_seq_length.\n",
        "    '''\n",
        "    #attention_mask = attention_mask.split(\" \")\n",
        "    attention_mask = attention_mask[:max_seq_length]\n",
        "    attention_mask = attention_mask + [0]*(max_seq_length - len(attention_mask))\n",
        "    return attention_mask\n",
        "\n",
        "\n",
        "def pad_token_type(token_type, max_seq_length=max_input_length):\n",
        "    ''' \n",
        "    Pads the token type to the max_seq_length.\n",
        "    '''\n",
        "    #token_type = token_type.split(\" \")\n",
        "    token_type = token_type[:max_seq_length]\n",
        "    token_type = token_type + [1]*(max_seq_length - len(token_type))\n",
        "    return token_type\n",
        "\n",
        "def split_and_cut(sentence):\n",
        "    tokens = sentence.strip().split(\" \")\n",
        "    tokens = tokens[:max_input_length]\n",
        "    return tokens\n",
        "\n",
        "def convert_list_to_str(token):\n",
        "    return ''.join(str(e) for e in token)\n",
        "    \n",
        "def convert_to_int(token):\n",
        "    return [int(x) for x in token]\n",
        "\n",
        "\n",
        "def preprocess_data_for_bert(path, max_seq_length=max_input_length, tokenizer=tokenizer):\n",
        "    ''' \n",
        "    Preprocesses the anli jsonl data for BERT.\n",
        "    '''\n",
        "    dataset = create_examples(path)\n",
        "    df = pd.DataFrame(columns=['label', 'sequence', 'attention_mask', 'token_type', 'sentence1', 'sentence2'])\n",
        "    for i, example in enumerate(dataset):\n",
        "        sent1 = tokenizer.tokenize(example.text_a)\n",
        "        sent1 = [cls_token] + sent1 + [sep_token]\n",
        "        sent2 = tokenizer.tokenize(example.text_b)\n",
        "        sent2 = sent2 + [sep_token]\n",
        "        final_sent = sent1 + sent2\n",
        "        label = example.label\n",
        "        attention_mask = [1]*len(final_sent)\n",
        "        #attention_mask = convert_list_to_str(attention_mask)\n",
        "        token_type = get_sent1_token_type(sent1)+ get_sent2_token_type(sent2)\n",
        "        #token_type = convert_list_to_str(token_type)\n",
        "        #final_sent = \" \".join(final_sent)\n",
        "        final_sent = pad_sequence(final_sent)\n",
        "        final_sent = tokenizer.convert_tokens_to_ids(final_sent)\n",
        "        attention_mask = pad_attention_mask(attention_mask)\n",
        "        token_type = pad_token_type(token_type)\n",
        "        df.loc[i] = [np.array(label_conversion[label]), np.array(final_sent), np.array(attention_mask), np.array(token_type), np.array(sent1), np.array(sent2)]\n",
        "        #df.loc[i] = [label, final_sent, attention_mask, token_type, sent1, sent2]\n",
        "\n",
        "    return df\n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YMnWwvlWIhIb"
      },
      "outputs": [],
      "source": [
        "df_T = preprocess_data_for_bert(\"./data/anli_v1.0/R1/train.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BTr7-nJWIhIc",
        "outputId": "57c1dd29-80db-48d6-f1f8-1123134e57fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sequence</th>\n",
              "      <th>attention_mask</th>\n",
              "      <th>token_type</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[101, 1996, 19177, 20820, 8286, 2291, 1006, 30...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[[CLS], the, parma, trolley, ##bus, system, (,...</td>\n",
              "      <td>[the, trolley, ##bus, system, has, over, 2, ur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10481, 18496, 2239, 19021, 3064, 2080, 1...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[[CLS], alexandra, lend, ##on, bas, ##ted, ##o...</td>\n",
              "      <td>[sha, ##rro, ##n, mac, ##rea, ##dy, was, a, po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10481, 18496, 2239, 19021, 3064, 2080, 1...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[[CLS], alexandra, lend, ##on, bas, ##ted, ##o...</td>\n",
              "      <td>[bas, ##ted, ##o, didn, ', t, keep, any, pets,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10481, 18496, 2239, 19021, 3064, 2080, 1...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[[CLS], alexandra, lend, ##on, bas, ##ted, ##o...</td>\n",
              "      <td>[alexandra, bas, ##ted, ##o, was, named, by, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>[101, 10481, 18496, 2239, 19021, 3064, 2080, 1...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[[CLS], alexandra, lend, ##on, bas, ##ted, ##o...</td>\n",
              "      <td>[bas, ##ted, ##o, cared, for, all, the, animal...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                           sequence  \\\n",
              "0     1  [101, 1996, 19177, 20820, 8286, 2291, 1006, 30...   \n",
              "1     0  [101, 10481, 18496, 2239, 19021, 3064, 2080, 1...   \n",
              "2     0  [101, 10481, 18496, 2239, 19021, 3064, 2080, 1...   \n",
              "3     0  [101, 10481, 18496, 2239, 19021, 3064, 2080, 1...   \n",
              "4     0  [101, 10481, 18496, 2239, 19021, 3064, 2080, 1...   \n",
              "\n",
              "                                      attention_mask  \\\n",
              "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
              "\n",
              "                                          token_type  \\\n",
              "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                           sentence1  \\\n",
              "0  [[CLS], the, parma, trolley, ##bus, system, (,...   \n",
              "1  [[CLS], alexandra, lend, ##on, bas, ##ted, ##o...   \n",
              "2  [[CLS], alexandra, lend, ##on, bas, ##ted, ##o...   \n",
              "3  [[CLS], alexandra, lend, ##on, bas, ##ted, ##o...   \n",
              "4  [[CLS], alexandra, lend, ##on, bas, ##ted, ##o...   \n",
              "\n",
              "                                           sentence2  \n",
              "0  [the, trolley, ##bus, system, has, over, 2, ur...  \n",
              "1  [sha, ##rro, ##n, mac, ##rea, ##dy, was, a, po...  \n",
              "2  [bas, ##ted, ##o, didn, ', t, keep, any, pets,...  \n",
              "3  [alexandra, bas, ##ted, ##o, was, named, by, h...  \n",
              "4  [bas, ##ted, ##o, cared, for, all, the, animal...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_T.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CoYADtdJIhId"
      },
      "outputs": [],
      "source": [
        "def convert_to_tuples(df):\n",
        "    '''\n",
        "    Converts the dataframe to list of tuples.\n",
        "    '''\n",
        "    ds = []\n",
        "    for index, row in df.iterrows():\n",
        "        ds.append((row['label'], row['sequence'], row['attention_mask'], row['token_type']))\n",
        "    \n",
        "    return ds\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dUzblASZIhId"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Elxu89LTIhIe"
      },
      "outputs": [],
      "source": [
        "class BertAnliProcessor():\n",
        "    \"\"\"Processor for the ANLI data set.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.train_data = preprocess_data_for_bert(data_dir+\"train.jsonl\")\n",
        "        self.dev_data = preprocess_data_for_bert(data_dir+\"dev.jsonl\")\n",
        "        self.test_data = preprocess_data_for_bert(data_dir+\"test.jsonl\")\n",
        "        self.train_data.to_csv(data_dir+\"train_processed.csv\", index=False)\n",
        "        self.dev_data.to_csv(data_dir+\"dev_processed.csv\", index=False)\n",
        "        self.test_data.to_csv(data_dir+\"test_processed.csv\", index=False)\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "        \"\"\"\n",
        "        Formats the train data into a DataLoader.\n",
        "        tuple : (label, sequence, attention_mask, token_type)\n",
        "        \"\"\"\n",
        "        ds = convert_to_tuples(self.train_data)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        return loader\n",
        "\n",
        "    def get_dev_dataloader(self):\n",
        "        \"\"\"\n",
        "        Formats the dev data into a DataLoader.\n",
        "        tuple : (label, sequence, attention_mask, token_type)\n",
        "        \"\"\"\n",
        "        ds = convert_to_tuples(self.dev_data)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        return loader\n",
        "\n",
        "\n",
        "    def get_test_dataloader(self):\n",
        "        \"\"\"\n",
        "        Formats the test data into a DataLoader.\n",
        "        tuple : (label, sequence, attention_mask, token_type)\n",
        "        \"\"\"\n",
        "        ds = convert_to_tuples(self.test_data)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        return loader\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v81s9htsIhIe"
      },
      "outputs": [],
      "source": [
        "obj = BertAnliProcessor('./data/anli_v1.0/R1/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XWulpMb2IhIe"
      },
      "outputs": [],
      "source": [
        "loader = obj.get_dev_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Se5NZVCGIhIf"
      },
      "outputs": [],
      "source": [
        "x = next(iter(loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jzuiqFT9IhIf",
        "outputId": "5600eded-0173-45d0-bac4-84cfe83ccc64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([2, 2, 2, 0, 1, 1, 1, 0, 1, 2, 2, 0, 2, 1, 2, 1]),\n",
              " tensor([[  101,  1996,  2857,  ...,     0,     0,     0],\n",
              "         [  101,  2568,  3238,  ...,     0,     0,     0],\n",
              "         [  101,  1996, 22300,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  6770,  2239,  ...,     0,     0,     0],\n",
              "         [  101,  8894,  7621,  ...,     0,     0,     0],\n",
              "         [  101,  1000, 29347,  ...,     0,     0,     0]]),\n",
              " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
              " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1],\n",
              "         [0, 0, 0,  ..., 1, 1, 1]])]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "from transformers import BertModel\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
        "\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from jutils import *\n",
        "\n",
        "## cubic\n",
        "# lowersize = 40\n",
        "# hiddensize = 6\n",
        "\n",
        "## Gaussian\n",
        "# lowersize = 20\n",
        "# hiddensize = 8\n",
        "\n",
        "## club vs l1out\n",
        "lowersize = 40\n",
        "hiddensize = 8\n",
        "\n",
        "class CLUBv2(nn.Module):  # CLUB: Mutual Information Contrastive Learning Upper Bound\n",
        "    def __init__(self, x_dim, y_dim, lr=1e-3, beta=0):\n",
        "        super(CLUBv2, self).__init__()\n",
        "        self.hiddensize = y_dim\n",
        "        self.version = 2\n",
        "        self.beta = beta\n",
        "\n",
        "    def mi_est_sample(self, x_samples, y_samples):\n",
        "        sample_size = y_samples.shape[0]\n",
        "        random_index = torch.randint(sample_size, (sample_size,)).long()\n",
        "\n",
        "        positive = torch.zeros_like(y_samples)\n",
        "        negative = - (y_samples - y_samples[random_index]) ** 2 / 2.\n",
        "        upper_bound = (positive.sum(dim=-1) - negative.sum(dim=-1)).mean()\n",
        "        # return upper_bound/2.\n",
        "        return upper_bound\n",
        "\n",
        "    def mi_est(self, x_samples, y_samples):  # [nsample, 1]\n",
        "        positive = torch.zeros_like(y_samples)\n",
        "\n",
        "        prediction_1 = y_samples.unsqueeze(1)  # [nsample,1,dim]\n",
        "        y_samples_1 = y_samples.unsqueeze(0)  # [1,nsample,dim]\n",
        "        negative = - ((y_samples_1 - prediction_1) ** 2).mean(dim=1) / 2.   # [nsample, dim]\n",
        "        return (positive.sum(dim=-1) - negative.sum(dim=-1)).mean()\n",
        "        # return (positive.sum(dim = -1) - negative.sum(dim = -1)).mean(), positive.sum(dim = -1).mean(), negative.sum(dim = -1).mean()\n",
        "\n",
        "    def loglikeli(self, x_samples, y_samples):\n",
        "        return 0\n",
        "\n",
        "    def update(self, x_samples, y_samples, steps=None):\n",
        "        # no performance improvement, not enabled\n",
        "        if steps:\n",
        "            beta = self.beta if steps > 1000 else self.beta * steps / 1000  # beta anealing\n",
        "        else:\n",
        "            beta = self.beta\n",
        "\n",
        "        return self.mi_est_sample(x_samples, y_samples) * self.beta\n",
        "\n",
        "club = CLUBv2(x_dim=10,y_dim = 10, beta=5e-3).to(device)\n",
        "\n",
        "class BERTNLIModel(nn.Module):\n",
        "    def __init__(self,\n",
        "\n",
        "                 bert_model,\n",
        "\n",
        "                 hidden_dim,\n",
        "\n",
        "                 output_dim,\n",
        "\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
        "        self.out = nn.Linear(embedding_dim, output_dim)\n",
        "    def forward(self, sequence, attn_mask, token_type):\n",
        "        # bert_output = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids= token_type)[1]\n",
        "        bert_output = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids= token_type)\n",
        "        embedded = bert_output[1]\n",
        "        output = self.out(embedded)\n",
        "        hidden_states = bert_output[2]\n",
        "        first_state = hidden_states[0]\n",
        "        last_state = hidden_states[-1]\n",
        "        return (output , first_state, last_state)\n",
        "\n",
        "  #defining model\n",
        "HIDDEN_DIM = 512\n",
        "# OUTPUT_DIM = len(LABEL.vocab)\n",
        "OUTPUT_DIM = 3\n",
        "# model = BERTNLIModel(bert_model,\n",
        "#                          HIDDEN_DIM,\n",
        "#                          OUTPUT_DIM,\n",
        "#                         ).to(device)\n",
        "\n",
        "model = BERTNLIModel(bert_model,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                        ).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.sagemaker because of the following error (look up to see its traceback):\ncannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1076\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/sagemaker/__init__.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtrainer_sm\u001b[39;00m \u001b[39mimport\u001b[39;00m SageMakerTrainer\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtraining_args_sm\u001b[39;00m \u001b[39mimport\u001b[39;00m SageMakerTrainingArguments, is_sagemaker_dp_enabled\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/sagemaker/trainer_sm.py:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/trainer.py:164\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m is_apex_available():\n\u001b[0;32m--> 164\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mapex\u001b[39;00m \u001b[39mimport\u001b[39;00m amp\n\u001b[1;32m    166\u001b[0m \u001b[39mif\u001b[39;00m is_datasets_available():\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/apex/__init__.py:13\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyramid\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msecurity\u001b[39;00m \u001b[39mimport\u001b[39;00m NO_PERMISSION_REQUIRED\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyramid\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msession\u001b[39;00m \u001b[39mimport\u001b[39;00m UnencryptedCookieSessionFactoryConfig\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyramid\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msettings\u001b[39;00m \u001b[39mimport\u001b[39;00m asbool\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39moptim\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m AdamW(model\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,eps\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m,correct_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1053\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1064\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_objects[name]\n\u001b[1;32m   1063\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules:\n\u001b[0;32m-> 1064\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(name)\n\u001b[1;32m   1065\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m   1066\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/utils/import_utils.py:1078\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1077\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1078\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1079\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1080\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1081\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.sagemaker because of the following error (look up to see its traceback):\ncannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)"
          ]
        }
      ],
      "source": [
        "from transformers import *\n",
        "import torch.optim as optim\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "# def get_scheduler(optimizer, warmup_steps):\n",
        "#     scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "#     return scheduler\n",
        "# criterion = nn.CrossEntropyLoss().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "# mp = True\n",
        "# if mp:\n",
        "#     try:\n",
        "#         from apex import amp\n",
        "#     except ImportError:\n",
        "#         raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "#     model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
        "\n",
        "def categorical_accuracy(preds, y):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "\n",
        "    correct = (max_preds.squeeze(1)==y).float()\n",
        "\n",
        "    return correct.sum() / len(y)\n",
        "\n",
        "max_grad_norm = 1\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "model.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'optimizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m label, sequence, attention_mask, token_type  \u001b[39m=\u001b[39m samples\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(label)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# print(sequence)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(attention_mask)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(token_type)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \u001b[39m# clear gradients first\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache() \u001b[39m# releases all unoccupied cached memory\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/nli_preprocess.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m sequence \u001b[39m=\u001b[39m sequence\u001b[39m.\u001b[39mto(device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "for batch_idx, samples in enumerate(loader):\n",
        "    label, sequence, attention_mask, token_type  = samples\n",
        "    # print(label)\n",
        "    # print(sequence)\n",
        "    # print(attention_mask)\n",
        "    # print(token_type)\n",
        "    optimizer.zero_grad() # clear gradients first\n",
        "    torch.cuda.empty_cache() # releases all unoccupied cached memory\n",
        "    sequence = sequence.to(device)\n",
        "    attn_mask = attention_mask.to(device)\n",
        "    token_type = token_type.to(device)\n",
        "    label = label.to(device)\n",
        "    predictions , firststate, laststate = model(sequence, attn_mask, token_type)\n",
        "    # print(firststate.size())\n",
        "    # print(laststate.size())\n",
        "    loss1 = criterion(predictions, label)\n",
        "    loss2= club.update(firststate, laststate)\n",
        "    loss = loss1 + loss2\n",
        "\n",
        "    acc = categorical_accuracy(predictions, label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss)\n",
        "    # scheduler.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "    torch.save(model.state_dict(),\"infobertr3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train(model, iterator, optimizer, criterion, scheduler):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    optimizer.zero_grad() # clear gradients first\n",
        "    torch.cuda.empty_cache() # releases all unoccupied cached memory\n",
        "    sequence = batch.sequence\n",
        "    attn_mask = batch.attention_mask\n",
        "    token_type = batch.token_type\n",
        "    label = batch.label\n",
        "    predictions = model(sequence, attn_mask, token_type)\n",
        "    loss = criterion(predictions, label)\n",
        "    acc = categorical_accuracy(predictions, label)\n",
        "    if mp:\n",
        "      with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
        "    else:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8814e103f48a9b9c45b3a0b04833ee73d900778b116d89f09aabf33b00c7043e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
