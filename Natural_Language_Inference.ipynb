{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHm8knglb6pJ",
        "outputId": "923535d8-d953-41bd-ab83-a72f15b7618c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.10.0 in /home/prayush/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
            "Requirement already satisfied: requests in /home/prayush/anaconda3/lib/python3.9/site-packages (from torchtext==0.10.0) (2.27.1)\n",
            "Requirement already satisfied: numpy in /home/prayush/anaconda3/lib/python3.9/site-packages (from torchtext==0.10.0) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /home/prayush/anaconda3/lib/python3.9/site-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /home/prayush/anaconda3/lib/python3.9/site-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in /home/prayush/anaconda3/lib/python3.9/site-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.10.0) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjZ8QK2TSfdl",
        "outputId": "88c502a3-727c-47d1-9ea8-758193732132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/prayush/anaconda3/lib/python3.9/site-packages (4.24.0)\n",
            "Requirement already satisfied: filelock in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/prayush/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/prayush/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/prayush/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/prayush/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1Jt-tMBiPfrI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "from transformers import DataProcessor, InputExample, InputFeatures\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n",
            "101 102 0 100\n",
            "512\n",
            "[tensor([2]), tensor([[  101,  2198,  2726,  5671,  1006,  2089,  1022,  1010, 12522,  1516,\n",
            "          2255,  2403,  1010,  6166,  1007,  2001,  1037,  9137,  1011,  2301,\n",
            "          3761,  1010,  5160,  1998,  3648,  2013,  3448,  1012,  2002,  2411,\n",
            "          3615,  2000,  2044,  1996,  2137,  2942,  2162,  2004,  1000,  3648,\n",
            "          5671,  1000,  1010,  2130,  2044,  2010,  2602,  2000,  3519,  1012,\n",
            "          2002,  2001,  1996,  2034,  5542,  1997,  2198,  2940,  1012,   102,\n",
            "          5671,  2001,  1037,  3648,  1999,  5374,   102,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]])]\n"
          ]
        }
      ],
      "source": [
        "bert_model_type = 'bert-base-uncased'\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_type)\n",
        "cls_token = tokenizer.cls_token\n",
        "sep_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "print(cls_token, sep_token, pad_token, unk_token)\n",
        "\n",
        "cls_token_idx = tokenizer.cls_token_id\n",
        "sep_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id\n",
        "print(cls_token_idx, sep_token_idx, pad_token_idx, unk_token_idx)\n",
        "\n",
        "label_conversion = {'n':0, #neutral\n",
        "'e':1, #entailment\n",
        "'c':2} #contradiction\n",
        "\n",
        "max_input_length = tokenizer.max_model_input_sizes[bert_model_type]\n",
        "print(max_input_length)\n",
        "\n",
        "# BATCH_SIZE = 16\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "def read_jsonl(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            lines = f.readlines()\n",
        "            return [json.loads(line) for line in lines]\n",
        "\n",
        "def create_examples(filename):\n",
        "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        data = read_jsonl(filename)\n",
        "        for (i, line) in enumerate(data):\n",
        "            guid = \"%s-%s\" % (\"anli-bert-tf\", i)\n",
        "            premise = line['context'] \n",
        "            hypothesis = line['hypothesis']\n",
        "            label = line['label']\n",
        "            examples.append(InputExample(guid=guid, text_a=premise, text_b=hypothesis, label=label))\n",
        "        return examples\n",
        "\n",
        "#Get list of 0s \n",
        "def get_sent1_token_type(sent):\n",
        "    try:\n",
        "        return [0]* len(sent)\n",
        "    except:\n",
        "        return []\n",
        "#Get list of 1s\n",
        "def get_sent2_token_type(sent):\n",
        "    try:\n",
        "        return [1]* len(sent)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def pad_sequence(sequence, max_seq_length=max_input_length, pad_token=pad_token):\n",
        "    ''' \n",
        "    Pads the sequence to the max_seq_length.\n",
        "    '''\n",
        "    #sequence = sequence.split(\" \")\n",
        "    sequence = sequence[:max_seq_length]\n",
        "    sequence = sequence + [pad_token]*(max_seq_length - len(sequence))\n",
        "    return sequence\n",
        "\n",
        "def pad_attention_mask(attention_mask, max_seq_length=max_input_length):\n",
        "    ''' \n",
        "    Pads the attention mask to the max_seq_length.\n",
        "    '''\n",
        "    #attention_mask = attention_mask.split(\" \")\n",
        "    attention_mask = attention_mask[:max_seq_length]\n",
        "    attention_mask = attention_mask + [0]*(max_seq_length - len(attention_mask))\n",
        "    return attention_mask\n",
        "\n",
        "\n",
        "def pad_token_type(token_type, max_seq_length=max_input_length):\n",
        "    ''' \n",
        "    Pads the token type to the max_seq_length.\n",
        "    '''\n",
        "    #token_type = token_type.split(\" \")\n",
        "    token_type = token_type[:max_seq_length]\n",
        "    token_type = token_type + [1]*(max_seq_length - len(token_type))\n",
        "    return token_type\n",
        "\n",
        "def split_and_cut(sentence):\n",
        "    tokens = sentence.strip().split(\" \")\n",
        "    tokens = tokens[:max_input_length]\n",
        "    return tokens\n",
        "\n",
        "def convert_list_to_str(token):\n",
        "    return ''.join(str(e) for e in token)\n",
        "    \n",
        "def convert_to_int(token):\n",
        "    return [int(x) for x in token]\n",
        "\n",
        "\n",
        "def preprocess_data_for_bert(path, max_seq_length=max_input_length, tokenizer=tokenizer):\n",
        "    ''' \n",
        "    Preprocesses the anli jsonl data for BERT.\n",
        "    '''\n",
        "    dataset = create_examples(path)\n",
        "    df = pd.DataFrame(columns=['label', 'sequence', 'attention_mask', 'token_type', 'sentence1', 'sentence2'])\n",
        "    for i, example in enumerate(dataset):\n",
        "        sent1 = tokenizer.tokenize(example.text_a)\n",
        "        sent1 = [cls_token] + sent1 + [sep_token]\n",
        "        sent2 = tokenizer.tokenize(example.text_b)\n",
        "        sent2 = sent2 + [sep_token]\n",
        "        final_sent = sent1 + sent2\n",
        "        label = example.label\n",
        "        attention_mask = [1]*len(final_sent)\n",
        "        #attention_mask = convert_list_to_str(attention_mask)\n",
        "        token_type = get_sent1_token_type(sent1)+ get_sent2_token_type(sent2)\n",
        "        #token_type = convert_list_to_str(token_type)\n",
        "        #final_sent = \" \".join(final_sent)\n",
        "        final_sent = pad_sequence(final_sent)\n",
        "        final_sent = tokenizer.convert_tokens_to_ids(final_sent)\n",
        "        attention_mask = pad_attention_mask(attention_mask)\n",
        "        token_type = pad_token_type(token_type)\n",
        "        df.loc[i] = [np.array(label_conversion[label]), np.array(final_sent), np.array(attention_mask), np.array(token_type), np.array(sent1), np.array(sent2)]\n",
        "        #df.loc[i] = [label, final_sent, attention_mask, token_type, sent1, sent2]\n",
        "\n",
        "    return df\n",
        "\n",
        "# !ls\n",
        "\n",
        "df_T = preprocess_data_for_bert(\"./data/anli_v1.0/anli_v1.0/R1/train.jsonl\")\n",
        "# df_T = preprocess_data_for_bert(\"./data/train.jsonl\")\n",
        "\n",
        "df_T.head()\n",
        "\n",
        "def convert_to_tuples(df):\n",
        "    '''\n",
        "    Converts the dataframe to list of tuples.\n",
        "    '''\n",
        "    ds = []\n",
        "    for index, row in df.iterrows():\n",
        "        ds.append((row['label'], row['sequence'], row['attention_mask'], row['token_type']))\n",
        "    \n",
        "    return ds\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "class BertAnliProcessor():\n",
        "    \"\"\"Processor for the ANLI data set.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        # self.dev_data = \n",
        "        self.train_data = preprocess_data_for_bert(data_dir+\"train.jsonl\")\n",
        "        # self.dev_data = preprocess_data_for_bert(data_dir+\"dev.jsonl\")\n",
        "        # self.test_data = preprocess_data_for_bert(data_dir+\"test.jsonl\")\n",
        "        self.train_data.to_csv(data_dir+\"train_processed.csv\", index=False)\n",
        "        # self.dev_data.to_csv(data_dir+\"dev_processed.csv\", index=False)\n",
        "        # self.test_data.to_csv(data_dir+\"test_processed.csv\", index=False)\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "        \"\"\"\n",
        "        Formats the train data into a DataLoader.\n",
        "        tuple : (label, sequence, attention_mask, token_type)\n",
        "        \"\"\"\n",
        "        ds = convert_to_tuples(self.train_data)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        return loader\n",
        "\n",
        "    def get_dev_dataloader(self):\n",
        "        \"\"\"\n",
        "        Formats the dev data into a DataLoader.\n",
        "        tuple : (label, sequence, attention_mask, token_type)\n",
        "        \"\"\"\n",
        "        ds = convert_to_tuples(self.dev_data)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        return loader\n",
        "\n",
        "\n",
        "    def get_test_dataloader(self):\n",
        "        \"\"\"\n",
        "        Formats the test data into a DataLoader.\n",
        "        tuple : (label, sequence, attention_mask, token_type)\n",
        "        \"\"\"\n",
        "        ds = convert_to_tuples(self.test_data)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        return loader\n",
        "\n",
        "obj = BertAnliProcessor('./data/anli_v1.0/anli_v1.0/R1/')\n",
        "\n",
        "loader = obj.get_train_dataloader()\n",
        "\n",
        "x = next(iter(loader))\n",
        "# for batch_idx, label, sequence, attention_mask, token_type in enumerate(loader):\n",
        "#     print(label)\n",
        "#     print(sequence)\n",
        "#     print(attention_mask)\n",
        "#     print(token_type)\n",
        "#     # print(batch_idx)\n",
        "#     # print(sequence)\n",
        "#     # print(attn_mask)\n",
        "#     # print(token_type)\n",
        "\n",
        "print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wos9wRvAazwP"
      },
      "source": [
        "# Pre-Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUfSq_3HBxr1"
      },
      "source": [
        "Converting the dataset into the form required by the pre-trained BERT-Base Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EQ2moHbZeJsq"
      },
      "outputs": [],
      "source": [
        "# using the same tokenizer used in pre-training\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F2hupELPbjHs"
      },
      "outputs": [],
      "source": [
        "# defining the maximum length of the input sentences\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "# defining the maximum length of each sentence\n",
        "max_sentence_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "s4vkRd-kfyaI"
      },
      "outputs": [],
      "source": [
        "# function to tokenize the sentences using BertTokenizer\n",
        "def tokenize_sentences(sentence):\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VfDcM7lygQBl"
      },
      "outputs": [],
      "source": [
        "# function to reduce the size of the sentence to the max_input_length\n",
        "def reduce_sentence_length(sentence):\n",
        "  tokens = sentence.strip().split(\" \")\n",
        "  tokens = tokens[:max_input_length]\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ynm9HvIohS6P"
      },
      "outputs": [],
      "source": [
        "# function to trim the sentence to the max_sentence_length\n",
        "def trim_sentence(sentence):\n",
        "  # splitting the sentence\n",
        "  sentence = sentence.split()\n",
        "  # check if the sentence has 128 or more tokens\n",
        "  if len(sentence) >= 128:\n",
        "    sentence = sentence[:max_sentence_length]\n",
        "  return \" \".join(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR6bxaCIkL39"
      },
      "source": [
        "Token type ids help the model to know which token belongs to which sentence. For tokens of the first sentence in input, token type ids contain 0 and for second sentence tokens, it contains 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AMrt9T5BiWZy"
      },
      "outputs": [],
      "source": [
        "# function to get the token type id's of the sentence-01\n",
        "def token_type_ids_sent_01(sentence):\n",
        "  try:\n",
        "    return [0] * len(sentence)\n",
        "  except:\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QkCm_LgLjkF-"
      },
      "outputs": [],
      "source": [
        "# function to get the token type id's of the sentence-02\n",
        "def token_type_ids_sent_02(sentence):\n",
        "  try:\n",
        "    return [1] * len(sentence)\n",
        "  except:\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQxl2MG2kW-5"
      },
      "source": [
        "Attention mask helps the model to know the useful tokens and padding that is done during batch preparation. Attention mask is basically a sequence of 1â€™s with the same length as input tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GcBk1wuUjy4S"
      },
      "outputs": [],
      "source": [
        "# function to get the attention mask of the given sentence\n",
        "def attention_mask_sentence(sentence):\n",
        "  try:\n",
        "    return [1] * len(sentence)\n",
        "  except:\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NKWyF-VSCJRp"
      },
      "outputs": [],
      "source": [
        "# function to combine the sequences from lists\n",
        "def combine_sequence(sequence):\n",
        "  return \" \".join(sequence)\n",
        "\n",
        "# function to combine the masks\n",
        "def combine_mask(mask):\n",
        "  mask = [str(m) for m in mask]\n",
        "  return \" \".join(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O40IOGrXk2c5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# trimming the sentences upto the maximum length\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_train[\u001b[39m'\u001b[39m\u001b[39msentence1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_train[\u001b[39m'\u001b[39m\u001b[39msentence1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(trim_sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_dev[\u001b[39m'\u001b[39m\u001b[39msentence1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_dev[\u001b[39m'\u001b[39m\u001b[39msentence1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(trim_sentence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_test[\u001b[39m'\u001b[39m\u001b[39msentence1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m'\u001b[39m\u001b[39msentence1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(trim_sentence)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ],
      "source": [
        "# trimming the sentences upto the maximum length\n",
        "df_train['sentence1'] = df_train['sentence1'].apply(trim_sentence)\n",
        "df_dev['sentence1'] = df_dev['sentence1'].apply(trim_sentence)\n",
        "df_test['sentence1'] = df_test['sentence1'].apply(trim_sentence)\n",
        "\n",
        "df_train['sentence2'] = df_train['sentence2'].apply(trim_sentence)\n",
        "df_dev['sentence2'] = df_dev['sentence2'].apply(trim_sentence)\n",
        "df_test['sentence2'] = df_test['sentence2'].apply(trim_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRVbFIvDmLZL"
      },
      "outputs": [],
      "source": [
        "# adding the [cls] and [sep] tokens\n",
        "df_train['t_sentence1'] = cls_token + ' ' + df_train['sentence1'] + ' ' + sep_token + ' '\n",
        "df_dev['t_sentence1'] = cls_token + ' ' + df_dev['sentence1'] + ' ' + sep_token + ' '\n",
        "df_test['t_sentence1'] = cls_token + ' ' + df_test['sentence1'] + ' ' + sep_token + ' '\n",
        "\n",
        "df_train['t_sentence2'] = df_train['sentence2'] + ' ' + sep_token\n",
        "df_dev['t_sentence2'] = df_dev['sentence2'] + ' ' + sep_token\n",
        "df_test['t_sentence2'] = df_test['sentence2'] + ' ' + sep_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4XTsmADppIi"
      },
      "outputs": [],
      "source": [
        "# applying the BertTokenizer to the newly generated sentences\n",
        "df_train['b_sentence1'] = df_train['t_sentence1'].apply(tokenize_sentences)\n",
        "df_dev['b_sentence1'] = df_dev['t_sentence1'].apply(tokenize_sentences)\n",
        "df_test['b_sentence1'] = df_test['t_sentence1'].apply(tokenize_sentences)\n",
        "\n",
        "df_train['b_sentence2'] = df_train['t_sentence2'].apply(tokenize_sentences)\n",
        "df_dev['b_sentence2'] = df_train['t_sentence2'].apply(tokenize_sentences)\n",
        "df_test['b_sentence2'] = df_test['t_sentence2'].apply(tokenize_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zqLwk69uOns"
      },
      "outputs": [],
      "source": [
        "# getting the token type ids for the sentences\n",
        "df_train['sentence1_token_type'] = df_train['b_sentence1'].apply(token_type_ids_sent_01)\n",
        "df_dev['sentence1_token_type'] = df_dev['b_sentence1'].apply(token_type_ids_sent_01)\n",
        "df_test['sentence1_token_type'] = df_test['b_sentence1'].apply(token_type_ids_sent_01)\n",
        "\n",
        "df_train['sentence2_token_type'] = df_train['b_sentence2'].apply(token_type_ids_sent_02)\n",
        "df_dev['sentence2_token_type'] = df_dev['b_sentence2'].apply(token_type_ids_sent_02)\n",
        "df_test['sentence2_token_type'] = df_test['b_sentence2'].apply(token_type_ids_sent_02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30yjp9Hb_FP5"
      },
      "outputs": [],
      "source": [
        "# obtain the seqence from the tokenized sentences\n",
        "df_train['sequence'] = df_train['b_sentence1'] + df_train['b_sentence2']\n",
        "df_dev['sequence'] = df_dev['b_sentence1'] + df_dev['b_sentence2']\n",
        "df_test['sequence'] = df_test['b_sentence1'] + df_test['b_sentence2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eByrSh3g_vSq"
      },
      "outputs": [],
      "source": [
        "# generating attention mask \n",
        "df_train['attention_mask'] = df_train['sequence'].apply(attention_mask_sentence)\n",
        "df_dev['attention_mask'] = df_dev['sequence'].apply(attention_mask_sentence)\n",
        "df_test['attention_mask'] = df_test['sequence'].apply(attention_mask_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yeI-mkeAeTY"
      },
      "outputs": [],
      "source": [
        "# combining the token type of both sentences\n",
        "df_train['token_type'] = df_train['sentence1_token_type'] + df_train['sentence2_token_type']\n",
        "df_dev['token_type'] = df_dev['sentence1_token_type'] + df_train['sentence2_token_type']\n",
        "df_test['token_type'] = df_test['sentence1_token_type'] + df_test['sentence2_token_type']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70vds15ERCKj"
      },
      "source": [
        "Dropping the rows with NaN Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtyGHO-PQsuh",
        "outputId": "b7b9e7f5-9820-4f27-f590-38f3b0300a81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(491, 12)"
            ]
          },
          "execution_count": 329,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dev.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHLBuUMrGD79",
        "outputId": "1c604854-6c69-4459-ad71-14ecec8ebb91"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_dev' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m Iterable\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m testing_sequence \u001b[39m=\u001b[39m df_dev[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m testing_sequence:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(i, Iterable):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_dev' is not defined"
          ]
        }
      ],
      "source": [
        "from collections.abc import Iterable\n",
        "testing_sequence = df_dev['sequence'].to_list()\n",
        "for i in testing_sequence:\n",
        "  if not isinstance(i, Iterable):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgQr0A9JQzjJ",
        "outputId": "0a6720fb-5872-480e-8de9-8a43ac4fc8e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(488, 12)"
            ]
          },
          "execution_count": 331,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_dev = df_dev.dropna(subset = ['sequence'])\n",
        "df_dev.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODwod50VBaUh"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Converting the inputs to sequential for torchtext Field\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_train[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_train[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(combine_sequence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_dev[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m]  \u001b[39m=\u001b[39m df_dev[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(combine_sequence)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df_test[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_test[\u001b[39m'\u001b[39m\u001b[39msequence\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(combine_sequence)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Converting the inputs to sequential for torchtext Field\n",
        "df_train['sequence'] = df_train['sequence'].apply(combine_sequence)\n",
        "df_dev['sequence']  = df_dev['sequence'].apply(combine_sequence)\n",
        "df_test['sequence'] = df_test['sequence'].apply(combine_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRUDCA1SNIkZ"
      },
      "outputs": [],
      "source": [
        "df_train['attention_mask'] = df_train['attention_mask'].apply(combine_mask)\n",
        "df_dev['attention_mask'] = df_dev['attention_mask'].apply(combine_mask)\n",
        "df_test['attention_mask'] = df_test['attention_mask'].apply(combine_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1ADW5UxNLvM"
      },
      "outputs": [],
      "source": [
        "df_train['token_type'] = df_train['token_type'].apply(combine_mask)\n",
        "df_dev['token_type'] = df_dev['token_type'].apply(combine_mask)\n",
        "df_test['token_type'] = df_test['token_type'].apply(combine_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxAlWpGuRQda"
      },
      "outputs": [],
      "source": [
        "# extracting the required columns\n",
        "df_train = df_train[['gold_label', 'sequence', 'attention_mask', 'token_type']]\n",
        "df_dev = df_dev[['gold_label', 'sequence', 'attention_mask', 'token_type']]\n",
        "df_test = df_test[['gold_label', 'sequence', 'attention_mask', 'token_type']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V29fUXPmSa5I"
      },
      "outputs": [],
      "source": [
        "# saving the data in the files\n",
        "df_train.to_csv('snli_1.0/snli_1.0_train.csv', index=False)\n",
        "df_dev.to_csv('snli_1.0/snli_1.0_dev.csv', index=False)\n",
        "df_test.to_csv('snli_1.0/snli_1.0_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLbWctToVfvU",
        "outputId": "5ce6a6b4-721d-40d1-de55-95a5b8c8ccc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Icon\t\t  snli_1.0_dev.jsonl  snli_1.0_test.jsonl  snli_1.0_train.jsonl\n",
            "README.txt\t  snli_1.0_dev.txt    snli_1.0_test.txt    snli_1.0_train.txt\n",
            "snli_1.0_dev.csv  snli_1.0_test.csv   snli_1.0_train.csv\n"
          ]
        }
      ],
      "source": [
        "!ls snli_1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YJawSAp1WBKv",
        "outputId": "7dc4e038-760f-4b23-d195-28799d5785aa"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_train\u001b[39m.\u001b[39mhead()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
          ]
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39d2gCQRWM1F"
      },
      "outputs": [],
      "source": [
        "# function to convert the attention_mask and token_type ids to int\n",
        "def convert_to_int(ids):\n",
        "  ids = [int(d) for d in ids]\n",
        "  return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWHrwSBaXaXA"
      },
      "source": [
        "Create PyTorch Tensor using torchtext field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o7oxnFwffNm"
      },
      "outputs": [],
      "source": [
        "# importing the saved data from csv file\n",
        "df_train = pd.read_csv('snli_1.0/snli_1.0_train.csv')\n",
        "df_dev = pd.read_csv('snli_1.0/snli_1.0_dev.csv')\n",
        "df_test = pd.read_csv('snli_1.0/snli_1.0_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk8oIomAWgpm"
      },
      "outputs": [],
      "source": [
        "from torchtext.legacy import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byJCOyBXXvmY"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# text field for sequence\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m TEXT \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mField(batch_first \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                   use_vocab \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                   tokenize \u001b[39m=\u001b[39m reduce_sentence_length,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                   preprocessing \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                   pad_token \u001b[39m=\u001b[39m pad_token_idx,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                   unk_token \u001b[39m=\u001b[39m unk_token_idx)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# label field for label \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m LABEL \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mLabelField()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "# text field for sequence\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = reduce_sentence_length,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "# label field for label \n",
        "LABEL = data.LabelField()\n",
        "# text field for attention mask\n",
        "ATTENTION = data.Field(batch_first = True,\n",
        "                       use_vocab = False,\n",
        "                       tokenize = reduce_sentence_length,\n",
        "                       preprocessing = convert_to_int,\n",
        "                       pad_token = pad_token_idx)\n",
        "# text field for token type ids\n",
        "TTYPE = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = reduce_sentence_length,\n",
        "                  preprocessing = convert_to_int,\n",
        "                  pad_token = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gny4P81goc5j"
      },
      "outputs": [],
      "source": [
        "fields = [('label', LABEL), ('sequence', TEXT), ('attention_mask', ATTENTION), ('token_type', TTYPE)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNmQMye5osNh"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_data, valid_data, test_data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mTabularDataset\u001b[39m.\u001b[39msplits(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                         path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msnli_1.0\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                         train \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msnli_1.0_train.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                         validation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msnli_1.0_dev.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                         test \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msnli_1.0_test.csv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                         \u001b[39mformat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                         fields \u001b[39m=\u001b[39m fields,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                         skip_header \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X56sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_data_len \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "                                        path = 'snli_1.0',\n",
        "                                        train = 'snli_1.0_train.csv',\n",
        "                                        validation = 'snli_1.0_dev.csv',\n",
        "                                        test = 'snli_1.0_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True)\n",
        "train_data_len = len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjmkgQwXqUuM"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# building the vocabulary for the labels\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m LABEL\u001b[39m.\u001b[39mbuild_vocab(train_data)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "# building the vocabulary for the labels\n",
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERxou_WiqqDP"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_iterator, valid_iterator, test_iterator \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mBucketIterator\u001b[39m.\u001b[39msplits(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     (train_data, valid_data, test_data), \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     batch_size \u001b[39m=\u001b[39m BATCH_SIZE,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     sort_key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msequence),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     sort_within_batch \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#X61sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     device \u001b[39m=\u001b[39m device)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ],
      "source": [
        "# using bucketiterator for preparing batches for training\n",
        "BATCH_SIZE = 16\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.sequence),\n",
        "    sort_within_batch = False, \n",
        "    device = device)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeGgTEHJBQTW"
      },
      "source": [
        "# Model Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_fpLN3zrXs2"
      },
      "source": [
        "Using the pre-trained Bert_Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "c8935f9b17f54efdaf5d09f04ed38ca3",
            "a6d74b122d91414f8467fc19dadf4b4f",
            "9db08a1e8ff141df94d5fe05ab67c57c",
            "7535d4ce819741f7b461973be672bbe1",
            "6370524ea1f34c19a061bbe64824b306",
            "18d90c01edcd40b9ac8af0678707fa41",
            "fe3d923ef196463992a0cb56cfb6027e",
            "fdf7b6a3f9ef473697913fb05905e212",
            "3753f354945a48d49aece7ed84de31e9",
            "2ca05de4c03440c8ae03b7afc1481ac0",
            "c74b6983cb6a478484507e99507ddf8d"
          ]
        },
        "id": "7SacmaUWrSnw",
        "outputId": "1b16724c-7308-4e6a-a7be-7494e0d65e41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hKuVbC-shKs"
      },
      "source": [
        "Using BERT architecture along with one linear layer for the output prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CLUBv2(nn.Module):  # CLUB: Mutual Information Contrastive Learning Upper Bound\n",
        "    def __init__(self, x_dim, y_dim, lr=1e-3, beta=0):\n",
        "        super(CLUBv2, self).__init__()\n",
        "        self.hiddensize = y_dim\n",
        "        self.version = 2\n",
        "        self.beta = beta\n",
        "\n",
        "    def mi_est_sample(self, x_samples, y_samples):\n",
        "        sample_size = y_samples.shape[0]\n",
        "        random_index = torch.randint(sample_size, (sample_size,)).long()\n",
        "\n",
        "        positive = torch.zeros_like(y_samples)\n",
        "        negative = - (y_samples - y_samples[random_index]) ** 2 / 2.\n",
        "        upper_bound = (positive.sum(dim=-1) - negative.sum(dim=-1)).mean()\n",
        "        # return upper_bound/2.\n",
        "        return upper_bound\n",
        "\n",
        "    def mi_est(self, x_samples, y_samples):  # [nsample, 1]\n",
        "        positive = torch.zeros_like(y_samples)\n",
        "\n",
        "        prediction_1 = y_samples.unsqueeze(1)  # [nsample,1,dim]\n",
        "        y_samples_1 = y_samples.unsqueeze(0)  # [1,nsample,dim]\n",
        "        negative = - ((y_samples_1 - prediction_1) ** 2).mean(dim=1) / 2.   # [nsample, dim]\n",
        "        return (positive.sum(dim=-1) - negative.sum(dim=-1)).mean()\n",
        "        # return (positive.sum(dim = -1) - negative.sum(dim = -1)).mean(), positive.sum(dim = -1).mean(), negative.sum(dim = -1).mean()\n",
        "\n",
        "    def loglikeli(self, x_samples, y_samples):\n",
        "        return 0\n",
        "\n",
        "    def update(self, x_samples, y_samples, steps=None):\n",
        "        # no performance improvement, not enabled\n",
        "        if steps:\n",
        "            beta = self.beta if steps > 1000 else self.beta * steps / 1000  # beta anealing\n",
        "        else:\n",
        "            beta = self.beta\n",
        "\n",
        "        return self.mi_est_sample(x_samples, y_samples) * self.beta\n",
        "\n",
        "club = CLUBv2(x_dim=10,y_dim = 10, beta=5e-3).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDeA_jrtrwMM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from packaging import version\n",
        "\n",
        "\n",
        "class BERTNLIModel(nn.Module):\n",
        "    def __init__(self, bert_model, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        embedding_dim = bert_model.config.to_dict()['hidden_size']\n",
        "        self.out = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, sequence, attn_mask, token_type):\n",
        "        embedded = self.bert(input_ids = sequence, attention_mask = attn_mask, token_type_ids = token_type)\n",
        "        embed = embedded[1]\n",
        "        outputs = self.out(embed)\n",
        "        hidden_states = embedded[2]  # need to set config.output_hidden = True\n",
        "        first_state = hidden_states[0]\n",
        "        last_state = hidden_states[-1]\n",
        "        \n",
        "        return (outputs, first_state, last_state)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY44q_Acs6eS"
      },
      "outputs": [],
      "source": [
        "# loading the model\n",
        "HIDDEN_DIM = 512\n",
        "# OUTPUT_DIM = len(LABEL.vocab)\n",
        "OUTPUT_DIM = 3\n",
        "model = BERTNLIModel(bert_model, HIDDEN_DIM, OUTPUT_DIM).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Kg_ev-tXQ2",
        "outputId": "3234ce76-2ebe-484c-c932-85724ee5d48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 109,484,547 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# function to count the parameters of the model\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p. requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36x2XVyfyD8H"
      },
      "source": [
        "Using the Apex Nvidia a PyTorch extension for mixed precision and distributed training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akrvpEzExtLe",
        "outputId": "6a47c5b3-5cd8-41ad-be5b-5052a941f7de"
      },
      "outputs": [],
      "source": [
        "# %%writefile setup.sh\n",
        "\n",
        "# git clone https://github.com/NVIDIA/apex\n",
        "# cd apex\n",
        "# pip install -v --disable-pip-version-check --no-cache-dir ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aECUC0Nhxwsm",
        "outputId": "c67676da-4a17-446d-9a57-d91f62b4eaad"
      },
      "outputs": [],
      "source": [
        "# !sh setup.sh\n",
        "\n",
        "# !pip install torch==1.4+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u7izxEaymdv"
      },
      "source": [
        "Defining the loss function and optimizer for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drBEd-qJyBA3",
        "outputId": "06f28855-eeff-47b1-ebbe-4ecb66b39043"
      },
      "outputs": [],
      "source": [
        "from transformers.optimization import *\n",
        "# from apex import amp\n",
        "import torch.optim as optim\n",
        "optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "# def get_scheduler(optimizer, warmup_steps):\n",
        "#     scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)\n",
        "#     return scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHFWUDHN2JhK"
      },
      "outputs": [],
      "source": [
        "# using the cross entropy loss\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k_kixVm3LQf",
        "outputId": "eb022a9b-18fa-43f4-b8f6-0443069d1946"
      },
      "outputs": [],
      "source": [
        "# fp16 = False\n",
        "\n",
        "# if fp16:\n",
        "#     try:\n",
        "#         from apex import amp\n",
        "#     except ImportError:\n",
        "#         raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "#     model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
        "\n",
        "###################################33 ONLINE CODE ########################################\n",
        "# USE_APEX = True\n",
        "\n",
        "# import os, sys, shutil\n",
        "# import time\n",
        "# import gc\n",
        "# from contextlib import contextmanager\n",
        "# from pathlib import Path\n",
        "# import random\n",
        "# import numpy as np, pandas as pd\n",
        "# from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "# @contextmanager\n",
        "# def timer(name):\n",
        "#     t0 = time.time()\n",
        "#     yield\n",
        "#     print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
        "\n",
        "\n",
        "# if USE_APEX:\n",
        "#             with timer('install Nvidia apex'):\n",
        "#                 # Installing Nvidia Apex\n",
        "#                 os.system('git clone https://github.com/NVIDIA/apex; cd apex; pip install -v --no-cache-dir' + \n",
        "#                           ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./')\n",
        "#                 os.system('rm -rf apex/.git') # too many files, Kaggle fails\n",
        "#                 from apex import amp\n",
        "#                 model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "\n",
        "# optimizer = AdamW(model.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "# # define model as bert model\n",
        "# model = BERTNLIModel(bert_model, HIDDEN_DIM, OUTPUT_DIM,).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elmWWaO3z8oQ"
      },
      "outputs": [],
      "source": [
        "# function to calculate the accuracy of model\n",
        "def accuracy(pred, y):\n",
        "    max_preds = pred.argmax(dim = 1, keepdim = True)\n",
        "    correct = (max_preds.squeeze(1)==y).float()\n",
        "    return correct.sum() / len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERTNLIModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_grad_norm = 1\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9201, grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 62\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m token_type \u001b[39m=\u001b[39m token_type\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m predictions , firststate, laststate \u001b[39m=\u001b[39m model(sequence, attn_mask, token_type)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(firststate.size())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# print(laststate.size())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss1 \u001b[39m=\u001b[39m criterion(predictions, label)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 62\u001b[0m in \u001b[0;36mBERTNLIModel.forward\u001b[0;34m(self, sequence, attn_mask, token_type)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, sequence, attn_mask, token_type):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(input_ids \u001b[39m=\u001b[39;49m sequence, attention_mask \u001b[39m=\u001b[39;49m attn_mask, token_type_ids \u001b[39m=\u001b[39;49m token_type)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     embed \u001b[39m=\u001b[39m embedded[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y115sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(embed)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1007\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1009\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1015\u001b[0m     embedding_output,\n\u001b[1;32m   1016\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1017\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1018\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1019\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1020\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1021\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1022\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1023\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1024\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1027\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    596\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m    605\u001b[0m         attention_mask,\n\u001b[1;32m    606\u001b[0m         layer_head_mask,\n\u001b[1;32m    607\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m         past_key_value,\n\u001b[1;32m    610\u001b[0m         output_attentions,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    614\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:489\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    478\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    479\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    487\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    490\u001b[0m         hidden_states,\n\u001b[1;32m    491\u001b[0m         attention_mask,\n\u001b[1;32m    492\u001b[0m         head_mask,\n\u001b[1;32m    493\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    494\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    495\u001b[0m     )\n\u001b[1;32m    496\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    498\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:419\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    411\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    418\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 419\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    420\u001b[0m         hidden_states,\n\u001b[1;32m    421\u001b[0m         attention_mask,\n\u001b[1;32m    422\u001b[0m         head_mask,\n\u001b[1;32m    423\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    424\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    425\u001b[0m         past_key_value,\n\u001b[1;32m    426\u001b[0m         output_attentions,\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    428\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    429\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:351\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    349\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    353\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1168\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1167\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1168\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for batch_idx, samples in enumerate(loader):\n",
        "    label, sequence, attention_mask, token_type  = samples\n",
        "    # print(label)\n",
        "    # print(sequence)\n",
        "    # print(attention_mask)\n",
        "    # print(token_type)\n",
        "    optimizer.zero_grad() # clear gradients first\n",
        "    # torch.cuda.empty_cache() # releases all unoccupied cached memory\n",
        "    sequence = sequence.to(device)\n",
        "    attn_mask = attention_mask.to(device)\n",
        "    token_type = token_type.to(device)\n",
        "    label = label.to(device)\n",
        "    predictions , firststate, laststate = model(sequence, attn_mask, token_type)\n",
        "    # print(firststate.size())\n",
        "    # print(laststate.size())\n",
        "    loss1 = criterion(predictions, label)\n",
        "    loss2= club.update(firststate, laststate)\n",
        "    loss = loss1 + loss2\n",
        "\n",
        "    acc = accuracy(predictions, label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss)\n",
        "    # scheduler.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "    torch.save(model.state_dict(),\"infobertr3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train(model, iterator, optimizer, criterion, scheduler):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    optimizer.zero_grad() # clear gradients first\n",
        "    torch.cuda.empty_cache() # releases all unoccupied cached memory\n",
        "    sequence = batch.sequence\n",
        "    attn_mask = batch.attention_mask\n",
        "    token_type = batch.token_type\n",
        "    label = batch.label\n",
        "    predictions = model(sequence, attn_mask, token_type)\n",
        "    loss = criterion(predictions, label)\n",
        "    acc = categorical_accuracy(predictions, label)\n",
        "    if mp:\n",
        "      with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        scaled_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
        "    else:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m38FnyXvCcYI"
      },
      "source": [
        "# Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNM9rNcM3nxz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "501\n",
            "Test Loss: 0.526 | Test Acc: 85.03%\n"
          ]
        }
      ],
      "source": [
        "# def evaluate(model, iterator, criterion):\n",
        "#     #print(iterator)\n",
        "#     epoch_loss = 0\n",
        "#     epoch_acc = 0\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for batch in iterator:\n",
        "#             sequence = batch.sequence\n",
        "#             attn_mask = batch.attention_mask\n",
        "#             token_type = batch.token_type\n",
        "#             labels = batch.label\n",
        "#             predictions = model(sequence, attn_mask, token_type)\n",
        "#             loss = criterion(predictions, labels)\n",
        "#             acc = accuracy(predictions, labels)\n",
        "#             epoch_loss += loss.item()\n",
        "#             epoch_acc += acc.item()\n",
        "#     return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "print(len(loader))\n",
        "\n",
        "def evaluate(model):\n",
        "    # make function to evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, samples in enumerate(loader):\n",
        "            label, sequence, attention_mask, token_type  = samples\n",
        "            sequence = sequence.to(device)\n",
        "            attn_mask = attention_mask.to(device)\n",
        "            token_type = token_type.to(device)\n",
        "            label = label.to(device)\n",
        "            predictions , firststate, laststate = model(sequence, attn_mask, token_type)\n",
        "            max_preds = predictions.argmax(dim = 1, keepdim = True)\n",
        "            correct += (max_preds.squeeze(1)==label).float().sum()\n",
        "            total += label.size(0)\n",
        "            loss = criterion(predictions, label)\n",
        "            acc = accuracy(predictions, label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        return epoch_loss / len(loader), epoch_acc / len(loader)\n",
        "\n",
        "# evaluate the model\n",
        "test_loss, test_acc = evaluate(model)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD3CgIPY3_CA",
        "outputId": "5aa26ee8-8abf-42f1-f69e-41d9f06f433d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_data_len' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 66\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y122sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m N_EPOCHS \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y122sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m warmup_percent \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y122sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m total_steps \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mceil(N_EPOCHS \u001b[39m*\u001b[39m train_data_len \u001b[39m*\u001b[39m \u001b[39m1.\u001b[39m\u001b[39m/\u001b[39mBATCH_SIZE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y122sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m warmup_steps \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(total_steps\u001b[39m*\u001b[39mwarmup_percent)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y122sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# scheduler = get_scheduler(optimizer, warmup_steps)\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data_len' is not defined"
          ]
        }
      ],
      "source": [
        "import math\n",
        "N_EPOCHS = 1\n",
        "\n",
        "warmup_percent = 0.2\n",
        "total_steps = math.ceil(N_EPOCHS * train_data_len * 1./BATCH_SIZE)\n",
        "warmup_steps = int(total_steps*warmup_percent)\n",
        "# scheduler = get_scheduler(optimizer, warmup_steps)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, scheduler)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'infobert-nli.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7CQ0qK84qQg",
        "outputId": "47a508bc-b9a4-44a4-eeef-e05f7acd952a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_iterator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb Cell 67\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y123sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m correct\u001b[39m/\u001b[39mtotal\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y123sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y123sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m evaluate(model, test_iterator, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/prayush/Natural_Language_Inference/anli/Natural_Language_Inference.ipynb#Y123sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Test Acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_iterator' is not defined"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('infobertr3'))\n",
        "\n",
        "# test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "# print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')\n",
        "\n",
        "\n",
        "# # make evaluatre function\n",
        "# def evaluate(model, iterator, criterion):\n",
        "#     epoch_loss = 0\n",
        "#     epoch_acc = 0\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for batch in iterator:\n",
        "#             sequence = batch.sequence\n",
        "#             attn_mask = batch.attention_mask\n",
        "#             token_type = batch.token_type\n",
        "#             labels = batch.label\n",
        "#             predictions = model(sequence, attn_mask, token_type)\n",
        "#             loss = criterion(predictions, labels)\n",
        "#             acc = accuracy(predictions, labels)\n",
        "#             epoch_loss += loss.item()\n",
        "#             epoch_acc += acc.item()\n",
        "#     return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    # make function to evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, samples in enumerate(loader):\n",
        "            label, sequence, attention_mask, token_type  = samples\n",
        "            sequence = sequence.to(device)\n",
        "            attn_mask = attention_mask.to(device)\n",
        "            token_type = token_type.to(device)\n",
        "            label = label.to(device)\n",
        "            predictions , firststate, laststate = model(sequence, attn_mask, token_type)\n",
        "            max_preds = predictions.argmax(dim = 1, keepdim = True)\n",
        "            correct += (max_preds.squeeze(1)==label).float().sum()\n",
        "            total += label.size(0)\n",
        "            loss = criterion(predictions, label)\n",
        "            acc = accuracy(predictions, label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            print(f'Epoch [{epoch+1}/{N_EPOCHS}], Step [{batch_idx+1}/{len(loader)}], Loss: {loss.item():.4f}, Accuracy: {acc.item()*100:.2f}%')\n",
        "        print(f'Accuracy of the model on the test set: {correct/total*100:.2f}%')\n",
        "    return correct/total\n",
        "\n",
        "# evaluate the model\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iM6Gd87awSW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# function to get the results on custom inputs\n",
        "def predict_inference(premise, hypothesis, model, device):\n",
        "\n",
        "    # appending the 'cls' and 'sep' tokens \n",
        "    premise = cls_token + ' ' + premise + ' ' + sep_token\n",
        "    hypothesis = hypothesis + ' ' + sep_token\n",
        "    \n",
        "    # tokenize the premise and hypothesis using bert tokenizer\n",
        "    tokenize_premise = tokenize_sentences(premise)\n",
        "    tokenize_hypothesis = tokenize_sentences(hypothesis)\n",
        "\n",
        "    # generate the token type ids of both premise and hypothesis\n",
        "    premise_token_type = token_type_ids_sent_01(tokenize_premise)\n",
        "    hypothesis_token_type = token_type_ids_sent_02(tokenize_hypothesis)\n",
        "    \n",
        "    # combining the tokenized premise and hypothesis to generate the sequence\n",
        "    indexes = tokenize_premise + tokenize_hypothesis\n",
        "    \n",
        "    # converting the sequence of tokens into token ids\n",
        "    indexes = tokenizer.convert_tokens_to_ids(indexes)\n",
        "\n",
        "    # combining the premise and hypothesis tokens ids\n",
        "    indexes_type = premise_token_type + hypothesis_token_type\n",
        "    \n",
        "    # generating the attention mask of the ids\n",
        "    attention_mask = token_type_ids_sent_02(indexes)\n",
        "    \n",
        "    # creating the pytorch tensors of indexes, indexes_type, attention_mask\n",
        "    indexes = torch.LongTensor(indexes).unsqueeze(0).to(device)\n",
        "    indexes_type = torch.LongTensor(indexes_type).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.LongTensor(attention_mask).unsqueeze(0).to(device)\n",
        "    \n",
        "    # predicting to get the judgements\n",
        "    prediction = model(indexes, attention_mask, indexes_type)\n",
        "    \n",
        "    prediction = prediction.argmax(dim=-1).item()\n",
        "    \n",
        "    return LABEL.vocab.itos[prediction]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DaoMdwtCcaLI",
        "outputId": "ac90e0a0-9580-43e4-bbaf-b054942b18a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'neutral'"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "premise = 'A black race car starts up in front of a crowd of people.'\n",
        "hypothesis = 'A man is driving down a lonely road.'\n",
        "\n",
        "predict_inference(premise, hypothesis, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CPyV8XGScd10",
        "outputId": "079feb8d-1330-4e14-a111-e7fe5c2288da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'neutral'"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "premise = 'A soccer game with multiple males playing.'\n",
        "hypothesis = 'Some men are playing a sport.'\n",
        "\n",
        "predict_inference(premise, hypothesis, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hObZYIcH8czH",
        "outputId": "d9aefe4d-117c-46c1-ea3e-f228da23df0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'neutral'"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "premise = 'A smiling costumed woman is holding an umbrella.'\n",
        "hypothesis = 'A happy woman in a fairy costume holds an umbrella.'\n",
        "\n",
        "predict_inference(premise, hypothesis, model, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Natural Language Inference.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8814e103f48a9b9c45b3a0b04833ee73d900778b116d89f09aabf33b00c7043e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18d90c01edcd40b9ac8af0678707fa41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ca05de4c03440c8ae03b7afc1481ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3753f354945a48d49aece7ed84de31e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6370524ea1f34c19a061bbe64824b306": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7535d4ce819741f7b461973be672bbe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ca05de4c03440c8ae03b7afc1481ac0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c74b6983cb6a478484507e99507ddf8d",
            "value": " 420M/420M [00:08&lt;00:00, 39.3MB/s]"
          }
        },
        "9db08a1e8ff141df94d5fe05ab67c57c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf7b6a3f9ef473697913fb05905e212",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3753f354945a48d49aece7ed84de31e9",
            "value": 440473133
          }
        },
        "a6d74b122d91414f8467fc19dadf4b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18d90c01edcd40b9ac8af0678707fa41",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fe3d923ef196463992a0cb56cfb6027e",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "c74b6983cb6a478484507e99507ddf8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8935f9b17f54efdaf5d09f04ed38ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6d74b122d91414f8467fc19dadf4b4f",
              "IPY_MODEL_9db08a1e8ff141df94d5fe05ab67c57c",
              "IPY_MODEL_7535d4ce819741f7b461973be672bbe1"
            ],
            "layout": "IPY_MODEL_6370524ea1f34c19a061bbe64824b306"
          }
        },
        "fdf7b6a3f9ef473697913fb05905e212": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe3d923ef196463992a0cb56cfb6027e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
